{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6bbcf-2b56-403f-88e5-86a621232913",
   "metadata": {},
   "source": [
    "# Financial Market Insight Assistant #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd4644-1308-4e1d-80a0-80c3db57e5bb",
   "metadata": {},
   "source": [
    "<p5> Objective: Build a local, automated system that ingests financial news and stock data daily, embeds it using HuggingFace embeddings, stores it in Chroma vector DB, and answers domain-specific queries via a LangChain RAG pipeline. Showcase DE orchestration, logging, CI/CD, and model drift detection.</p5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f82571-7860-4392-8abb-b2e240740d45",
   "metadata": {},
   "source": [
    "<h3>Create an API Key</h3>\n",
    "<ul>\n",
    "    <li>Go to NEWSAPI.org and signup <a>https://newsapi.org/</a></li>\n",
    "    <li>Create a file with .env extention and save the NEWSAPI_KEY</li>\n",
    "    <li>NewsAPI is a RESTful API that provides real-time and historical news articles from thousands of sources worldwide. You can query news based on</li>\n",
    "    <li>It’s not a scraping tool—it’s a structured API with ready-to-use JSON responses.</li>\n",
    "    <li>JSON Responses include [Source, Author, Title, Desciption, URL]</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c61f76-5778-4bfb-b296-4240ae9caf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key has been successfully loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "if API_KEY:\n",
    "    print(\"The key has been successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90ee5f2-f3fa-4165-94c5-8d08e28cdb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "#--------- Paths ---------\n",
    "BASE_DIR = Path().resolve().parent\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "STOCK_DIR = BASE_DIR / \"data\" / \"stock\"\n",
    "DB_PATH = BASE_DIR / \"chroma_db.sqlite\"\n",
    "\n",
    "# Make sure directories exist\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STOCK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#--------- Connect to DB ---------\n",
    "connection_object = sqlite3.connect(DB_PATH)\n",
    "cursor = connection_object.cursor()\n",
    "\n",
    "#--------- Create stock_prices table if not exists ---------\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "    Datetime DATETIME,\n",
    "    Ticker TEXT,\n",
    "    Open REAL,\n",
    "    High REAL,\n",
    "    Low REAL,\n",
    "    Close REAL,\n",
    "    Volume INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "# INTEGER --> Whole Numbers\n",
    "# REAL --> Floating Point Numbers\n",
    "# TEXT --> String\n",
    "# DATETIME --> Date&Time    {These datatypes are sqlite3 specific}\n",
    "cursor.execute(create_table_query)\n",
    "connection_object.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58b3728-de18-433d-af88-17d1510f01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- fetching news articles --------------\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta, timezone\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def fetch_news():\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": \"NSE OR BASE OR FinTech\", # Query --> Free-text search, supports keywords, phrases, and Boolean operators (OR, AND).\n",
    "        \"language\": \"en\",\n",
    "        \"pageSize\": 20, # Maximum number of articles returned per request (1–100).\n",
    "        \"sortBy\": \"publishedAt\", # Detetermines order of articles. Other Options include\n",
    "        # ['relevancy', 'popularity', 'publishedAt'---> Newest First]\n",
    "        \"apiKey\": API_KEY, \n",
    "    }\n",
    "    response = requests.get(url, params=params, timeout=30)\n",
    "    data = response.json() # The data is extracted from the API\n",
    "    \n",
    "    \n",
    "    timestamp = datetime.datetime.now(timezone.utc).strftime(\"%Y%m%d%H\") # Year-Month-Day\n",
    "    filename = os.path.join(RAW_DIR, f\"news_{timestamp}.json\")\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf8\") as fp:\n",
    "        json.dump(data, fp) # Extracted Data is dumped inside the json file\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e8baf-201a-4843-99c8-2f67b04b2080",
   "metadata": {},
   "source": [
    "<h4>Fetching Stock data from yfinance</h4>\n",
    "<ul>\n",
    "    <li>yfinance is a Python library that allows you to fetch historical and real-time stock market data from Yahoo Finance.</li>\n",
    "    <li>It is a library not an API</li>\n",
    "    <li>Covers global exchanges and stock prices</li>\n",
    "    <li>Returns data as pandas DataFrame</li>\n",
    "</ul>\n",
    "<h5>Key Concepts</h5>\n",
    "<ul>\n",
    "    <li>ticker : Unique identifier of each stock</li>\n",
    "    <li>Info : Metadata about the company (sector, market cap etc...).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df41888-44de-4ef7-925d-89602a245b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_stocks(tickers=[\"HDFCBANK.NS\",\"ICICIBANK.NS\",\"SBIN.NS\"]):\n",
    "#     for t in tickers:\n",
    "#         df = yf.download(t, period=\"7d\", interval=\"1h\", group_by='ticker')\n",
    "        \n",
    "#         # --- Force tidy format ---\n",
    "#         if isinstance(df.columns, pd.MultiIndex):\n",
    "#             df = df.stack(level=1).rename_axis(['Datetime','Ticker']).reset_index()\n",
    "#         else:\n",
    "#             df.reset_index(inplace=True)\n",
    "#             df[\"Ticker\"] = t\n",
    "\n",
    "#         # --- Ensure column order ---\n",
    "#         df = df[['Datetime','Ticker','Open','High','Low','Close','Volume']]\n",
    "        \n",
    "#         df.to_sql(\"stock_prices\", connection_object, if_exists=\"append\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e48fac78-8193-4f6d-b8de-18233fc6fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---- fetching stock data using yfinance -----------------\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "\n",
    "# def fetch_stocks(tickers=[\"HDFCBANK.NS\",\"ICICIBANK.NS\",\"SBIN.NS\"]):\n",
    "#     for x in tickers:\n",
    "#         df = yf.download(x, period=\"7d\", interval=\"1h\")\n",
    "#         df.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in df.columns]\n",
    "#         df.reset_index(inplace=True)\n",
    "#         df[\"ticker\"] = x # Since df return [Open High...Volume], No description about company name\n",
    "        \n",
    "#         # The above step ensures the name of the company in each row.\n",
    "#         # Datetime     Open   High    Low   Close   Volume     ticker\n",
    "#         # 2025-09-18   09:15:00  1570.5  1581.0  1565.0  1578.2   1200000  HDFCBANK.NS\n",
    "#         # 2025-09-18   10:15:00  1578.3  1585.5  1572.0  1581.3    980000  HDFCBANK.NS\n",
    "#         df.to_sql(\"stock_prices\", connection_object, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af5f0c8-bb6b-48f4-85a8-8c3ac59826c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# import pandas as pd\n",
    "\n",
    "# def fetch_stock_single(ticker_symbol=\"AAPL\", period=\"7d\", interval=\"1h\"):\n",
    "#     ticker = yf.Ticker(ticker_symbol)\n",
    "#     df = ticker.history(period=period, interval=interval)\n",
    "    \n",
    "#     df.reset_index(inplace=True)          \n",
    "#     df[\"Ticker\"] = ticker_symbol          \n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61111e44-647e-4031-894f-408f6ca1c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_stocks(tickers=[\"HDFCBANK.NS\",\"ICICIBANK.NS\",\"SBIN.NS\"]):\n",
    "    full_df = pd.DataFrame()\n",
    "    \n",
    "    for t in tickers:\n",
    "        ticker = yf.Ticker(t)\n",
    "        df = ticker.history(period=\"7d\", interval=\"1h\")\n",
    "        df.reset_index(inplace=True)\n",
    "        df[\"Ticker\"] = t                 # Standard column name for all tickers\n",
    "        full_df = pd.concat([full_df, df], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f404a8c-b541-4a7c-8352-c4f6e45e6b7a",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>fetch_news() pulls all articles and dumps the raw API response into a JSON file inside data/raw/</li>\n",
    "    <li>That’s the archive or backup form of data</li>\n",
    "    <li>The below function reads back that raw JSON file. Extracts just a lightweight subset of fields (source, title, published date, path to raw file) and stores those into SQL.</li>\n",
    "    <li>Now we can query it like a database</li>\n",
    "    <li>e.g: SELECT title, publishedAt FROM news_meta</li>\n",
    "    <li>This helps in Exploration and Monioring</li>\n",
    "    <li>Debugging/Auditing</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a9b022-3b2a-4c70-a85a-9f196889ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_news_metadata(news_json_path):\n",
    "    with open(news_json_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    articles = data.get(\"articles\", [])\n",
    "    meta = []\n",
    "    for a in articles:\n",
    "        meta.append((\n",
    "            a.get(\"source\", {}).get(\"name\"),\n",
    "            a.get(\"title\"),\n",
    "            a.get(\"publishedAt\"),\n",
    "            news_json_path\n",
    "        ))\n",
    "    df = pd.DataFrame(meta, columns=[\"source\", \"title\", \"publishedAt\", \"raw_file\"])\n",
    "    df.to_sql(\"news_meta\", connection_object, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c29e7d-d384-445a-a6b0-5801364566e6",
   "metadata": {},
   "source": [
    "<p>\n",
    "json_response = {\n",
    "    \"articles\": [\n",
    "        {\"source\": {\"name\": \"CNN\"}, \"title\": \"Stocks Up\", \"publishedAt\": \"2025-09-25\"},\n",
    "        {\"source\": {\"name\": \"BBC\"}, \"title\": \"Market Crash\", \"publishedAt\": \"2025-09-24\"}\n",
    "    ]\n",
    "}\n",
    "</\n",
    "    p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b4948c1-77c9-4855-b2c7-81fa7c63982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    news_file = fetch_news() # returns a file \n",
    "    store_news_metadata(news_file) \n",
    "    fetch_stocks() \n",
    "    connection_object.close() \n",
    "    print(\"Data ingestion completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26324a35-41bd-4343-aac5-de12d8bec351",
   "metadata": {},
   "source": [
    "<h5>Preprocessing & Chunking News Articles</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda7642a-1dca-406f-9cba-fa43737bce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json_files = [x for x in os.listdir(RAW_DIR) if x.endswith(\".json\")]\n",
    "\n",
    "for file in raw_json_files:\n",
    "    filepath = os.path.join(RAW_DIR, file)\n",
    "    with open(filepath, \"r\", encoding=\"utf8\") as fp:\n",
    "        data = json.load(fp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f7b962-0b0f-41d4-95b6-c86ac8c6adc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d89c3f-8310-455b-acc4-8746132bac14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status', 'totalResults', 'articles'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcbad6f-0ab3-424a-95ee-af0cb2f117b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29884"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"totalResults\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39de7a13-f6a9-4fb0-a559-961c475f6fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': {'id': 'the-times-of-india', 'name': 'The Times of India'},\n",
       " 'author': 'Nandini Sanyal',\n",
       " 'title': 'Indian equities still expensive despite correction: Raunak Onkar, PPFAS',\n",
       " 'description': 'PPFAS Mutual Fund is adopting a cautious stance amid stretched valuations in Indian markets, favoring selective investments in private banks, auto stocks, and international equities. The fund house emphasizes holding higher cash levels, awaiting better opport…',\n",
       " 'url': 'https://economictimes.indiatimes.com/markets/expert-view/indian-equities-still-expensive-despite-correction-raunak-onkar-ppfas/articleshow/124114852.cms',\n",
       " 'urlToImage': 'https://img.etimg.com/thumb/msid-124114897,width-1200,height-630,imgsize-41888,overlay-etmarkets/articleshow.jpg',\n",
       " 'publishedAt': '2025-09-25T10:36:39Z',\n",
       " 'content': 'Indian markets may be going through a correction phase, but valuations remain stretched, making fresh investments tricky, according to Raunak Onkar, Research Head &amp; Fund Manager at PPFAS Mutual F… [+3519 chars]'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = data.get(\"articles\", [])\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34cbd50-a117-4f3b-8fb8-30625a201926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CHUNK_DIR = BASE_DIR / \"data\" / \"chunks\"\n",
    "CHUNK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clean_text(text:str):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # removing HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    # removing multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3820799-0e95-4b09-946d-db1ff765a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for x in range(0, len(words), chunk_size-overlap):\n",
    "        chunk = words[x:x+chunk_size]\n",
    "        if len(chunk)<30:\n",
    "            continue\n",
    "        chunk_text = \" \".join(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db62b08f-1b76-49d5-aa81-34b08f44985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news_file(news_json_path):\n",
    "    with open(news_json_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    articles = data.get(\"articles\", [])\n",
    "    chunk_records = []\n",
    "\n",
    "    for art in articles:\n",
    "        source = art.get(\"source\", {}).get(\"name\")\n",
    "        title = clean_text(art.get(\"title\", \"\"))\n",
    "        content = clean_text(art.get(\"content\", \"\")) or clean_text(art.get(\"description\", \"\"))\n",
    "\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # chunk the article\n",
    "        chunks = chunk_text(content)\n",
    "\n",
    "        for i, ch in enumerate(chunks):\n",
    "            chunk_records.append({\n",
    "                \"source\": source,\n",
    "                \"title\": title,\n",
    "                \"publishedAt\": art.get(\"publishedAt\"),\n",
    "                \"chunk_id\": f\"{title[:30]}_{i}\",\n",
    "                \"text\": ch,\n",
    "                \"raw_file\": str(news_json_path)\n",
    "            })\n",
    "\n",
    "    # save chunked output\n",
    "    if chunk_records:\n",
    "        df = pd.DataFrame(chunk_records)\n",
    "        ts = os.path.basename(news_json_path).replace(\".json\", \"\")\n",
    "        out_path = CHUNK_DIR / f\"chunks_{ts}.jsonl\"\n",
    "        df.to_json(out_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "        print(f\"Saved chunks → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6419b402-0691-47d4-ab8b-2db5d530377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks → C:\\Users\\DELL\\Data Science\\Deep Learning\\NLP\\Projects\\Financial_Insights\\data\\chunks\\chunks_news_2025092610.jsonl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    news_files = [x for x in os.listdir(RAW_DIR) if x.endswith(\".json\")]\n",
    "    for nf in news_files:\n",
    "        full_path = RAW_DIR / nf\n",
    "        process_news_file(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301031d-6454-443f-bc55-dcd55f2fc799",
   "metadata": {},
   "source": [
    "<h3>Generate Embeddings & Store in Chroma</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067b6bf5-2a7e-4fd6-ad0f-8a0468a90ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_chunks(jsonl_path):\n",
    "    df = pd.read_json(jsonl_path, lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7386a3d-4d23-4f8b-97e4-647cf290d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def create_chroma_vectorstore(df, persist_dir):\n",
    "    texts = df[\"text\"].tolist()\n",
    "    metadatas = df.drop(columns=[\"text\"]).to_dict(orient=\"records\")\n",
    "\n",
    "    vectordb = Chroma.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_model,\n",
    "        metadatas=metadatas,\n",
    "        persist_directory=str(persist_dir)\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(f\"Saved vectorstore at {persist_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19821572-6f76-455b-950d-6a69d48eba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vectorstore at C:\\Users\\DELL\\Data Science\\Deep Learning\\NLP\\Projects\\Financial_Insights\\chroma_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_8524\\4154980578.py:13: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chunk_files = sorted(CHUNK_DIR.glob(\"*.jsonl\"))\n",
    "    for cf in chunk_files:\n",
    "        df = load_chunks(cf)\n",
    "        create_chroma_vectorstore(df, CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8429f-b318-4f2f-88ad-540496d2411a",
   "metadata": {},
   "source": [
    "<h3>Retrieval Augmented Generation (RAG) with Ollama</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89d411c8-e655-4e38-a380-03b5eabdc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "# RetrievalQA = auto handles retrieval + context injection (less boilerplate than manual LLMChain(context))\n",
    "\n",
    "\n",
    "# ---- Load vector store ----\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# ---- Set up retriever ----\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\"k\": 3}   # fetch top 3 relevant chunks\n",
    ")\n",
    "\n",
    "# ---- Load Ollama LLM ----\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# ---- Build RetrievalQA chain ----\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True # return_source_documents=True means,\n",
    "    #along with the answer, also return the documents that were retrieved and used as context.\n",
    ")\n",
    "\n",
    "# ---- Example query ----\n",
    "query = \"What are the recent developments in the Indian banking sector?\"\n",
    "response = qa_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e638a9df-b967-4776-bff5-96900e8b495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "  As of my last update, there have been several significant developments in the Indian banking sector. Here are a few key points:\n",
      "\n",
      "1. Digital Transformation: Banks like HDFC Bank, ICICI Bank, and Axis Bank have been investing heavily in digital technology to enhance customer experience, increase efficiency, and reduce costs. This includes the adoption of AI, machine learning, blockchain, and other advanced technologies.\n",
      "\n",
      "2. Consolidation: The Indian government is pushing for consolidation among public sector banks to create fewer but stronger entities. In October 2020, five state-owned banks merged into one, creating the country's third-largest lender, Punjab National Bank (PNB).\n",
      "\n",
      "3. Regulatory Measures: The Reserve Bank of India (RBI) has implemented several regulatory measures aimed at strengthening the banking sector. These include stricter capital adequacy norms, stress tests for banks, and measures to combat fraud and non-performing assets.\n",
      "\n",
      "4. Fintech Collaborations: Indian banks have been partnering with fintech companies to introduce innovative products and services. For instance, Paytm Payments Bank has partnered with ICICI Bank to offer insurance products to its customers.\n",
      "\n",
      "5. Government Initiatives: The government launched the Pradhan Mantri Jan Dhan Yojana (PMJDY) in 2014, a financial inclusion mission aimed at providing banking services to every household in the country. As of March 2021, over 438 million accounts have been opened under this scheme.\n",
      "\n",
      "6. COVID-19 Response: Indian banks have been playing a crucial role in providing relief measures during the COVID-19 pandemic. This includes loan moratoriums, fee waivers, and restructuring of loans for affected customers.\n",
      "\n",
      "Sources:\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:\\n\", response[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe377e0a-8c13-4a70-a8ec-45f38e248f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/DELL/Data Science/Deep Learning/NLP/Projects/Financial_Insights')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2660c023-1ab0-4550-88e3-4c99c1df42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os - built-in, no version\n",
      "json - built-in, no version\n",
      "datetime - built-in, no version\n",
      "sqlite3 - built-in, no version\n",
      "pathlib - built-in, no version\n",
      "requests 2.32.5\n",
      "yfinance 0.2.66\n",
      "pandas 2.2.2\n",
      "langchain 0.3.27\n",
      "langchain_community 0.3.29\n",
      "chromadb 1.1.0\n",
      "transformers 4.53.2\n",
      "python-dotenv 1.1.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import sqlite3\n",
    "import requests\n",
    "import yfinance\n",
    "import pandas\n",
    "from dotenv import load_dotenv\n",
    "import langchain\n",
    "import langchain_community\n",
    "import chromadb\n",
    "import transformers\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Print versions\n",
    "print(\"os - built-in, no version\")\n",
    "print(\"json - built-in, no version\")\n",
    "print(\"datetime - built-in, no version\")\n",
    "print(\"sqlite3 - built-in, no version\")\n",
    "print(\"pathlib - built-in, no version\")\n",
    "\n",
    "print(\"requests\", requests.__version__)\n",
    "print(\"yfinance\", yfinance.__version__)\n",
    "print(\"pandas\", pandas.__version__)\n",
    "#print(\"python-dotenv\", dotenv.__version__)\n",
    "print(\"langchain\", langchain.__version__)\n",
    "print(\"langchain_community\", langchain_community.__version__)\n",
    "print(\"chromadb\", chromadb.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"python-dotenv\", importlib.metadata.version(\"python-dotenv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112ffcb-aced-493f-bd3f-cf42baf4567d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
